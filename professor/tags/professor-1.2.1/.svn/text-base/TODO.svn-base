Professor TODO
==============

For 1.2.2:
----------

* Use latest lighthisto.py and require Rivet >= 1.5.0 (for path searching
  compatibility)

* Sphinx docs updates:
    - prof-checkspace
    - prof-compare-tunes
    - prof-plotpulls
    - prof-showipol
    - prof-showminresults
    - prof-terror

* Allow prof-plotcorrelations to operate at any parameter point, using the same
  options flags as prof-sensitivities, and rename as prof-correlations (we don't
  call the other ``prof-plotsensitivities``, and simplicity is good).

* Do automatic extra bin stripping cf. recent versions of compare-histos

* Use updated Rivet path searching routines to find ref files, .plot files, etc.

* Use simpler non-SciPy method from David Grellscheid to make run combinations

* prof-I: show overall (weighted, via CL option) chi2 as well as unweighted per-plot chi2

* --manual-sp should automatically turn on the manual starting point, not just specify an unused one :)

* scanchi2 should be able to plot chi2s from ipol

* ratio plots in prof-I? (Or, due to space constraints, toggle to view the plot
  as a ratio, if there is ref data)

* Display options in prof-envelopes: show individual MC run lines, show
  density-shaded envelopes, show multiple overlaid CLs.


For 1.3.0:
----------

* Write TeX parser to sanitize TeX -> text strings, e.g. getting rid of markup
  like ``\,``, ``\mathrm``, ``\text``, etc.

* Print out log-intervalled estimation of time remaining

* How to handle correlated systematic errors?

* Reinstate --limits to auto-detect the param limits of the sample points which
  were actually used for that parameterisation. Store this info on the ipol
  object (along with the min and max of each bin -- see below).

* prof-I: display a background envelope calculated from the anchor points. Are
  the max and min stored in the bin ipol object? We should do so.

* Use WeightManager for "epsilon" errors... multiply on MC or ref error? Needs
  to be optional, and I think should also be overrideable by a single global
  epsilon factor specified in the scripts, so that users don't have to make the
  epsilon weights file if they don't want to use it for anything non-trivial. =>
  ANDY

* Make API more pleasant... focus on consistency, removing aliases/synonyms,
  dropping "get" prefixes, and making the names snappier. => ANDY



For 1.4.0:
----------

* Higher-order polynomials: provide 4th and 5th order for more generic
  parameterisation tasks. Probably requires splitting
  professor.interpolation.interpolation module, which is already too large.

* --eigentunes-dgof should accept either a fixed delta or a %/x of GoF_min.

* Check Python 2to3 compatibility.

* Finish error bands study! 3-param JIMMY tune -> 100 "stat" smearings around
  each of 100 "sys" minimisation points, using many different interpolations.

* WeightManager should provide a wm.getValue("/path/to/MYHIST:42") if at all possible.

* Provide more structured histo loading from multiple formats via functions in
  a professor.histo.input module.



For release after that:
-----------------------

* Parameterising the MC errors? Any use-case or does the median anchor point
  error do a good enough job?

* Develop a method to deal with +/- asymmetries and differences between
  eigenvectors for n_sigma ~> 1 in eigentunes (does minuit calc for n_sigma ~
  epsilon?)

* Diversify from the SVD mechanism for ipol generation. Neural networks are
  quite promising: we can use e.g. the PyROOT interface to TMVA, and this would
  give a more parameterisation-free result. Might be slower, and won't be as
  deterministic, but it would be a powerful extra feature. => ANDY

* Think about generalising the weights file to allow more things than just
  weights to be specified: the value could be a dict to also use different
  parameterisations for different histos/regions. In this model, the epsilon
  errors and so-on would also live in this one file rather than a separate one
  with the scalar format used for weights.

* Boundary sampling: generate samplings on the walls and corners of the space to
  constrain away parameterisation deviations outside the sampled region. We
  would need to keep these runs separate, so that they could all be used in all
  ipol buildings, with the runcomb sampling only happening to the "bulk" points.

* Use new style string formatting and "with ... as foo:" file handling: requires
  Python >= what version? Use from __future__ import ... mechanism or require
  Python 2.6... I guess the former for now.

* Clean ResultList interface.

* Modify prof-plotpulls to plot observable comparison plots comparing ipols to
  anchor points, as a way of checking ipol performance without having to do new
  line scans.

* Use clearer vetoing for inputs to prof-sensitivities: should we exclude bins
  with few stats or leave that up to the user via the prof-sensitivities interface?

* Bin-bin correlations: correlations between bins within an observable will
  influence the GoF, but we don't know exactly how. We suspect some effect on
  the error2 calc, i.e. bin_i.err**2 -> bin_i.err * bin_j.err or similar. But
  how does this affect the number of DoF? Presumably correlations mean that
  there are not the number of independent DoF weights that we were previously
  assuming. And how do bin weights enter if weight_i != weight_j? All stuff for
  the MSc student to think about!

  What we do NOW is to add an observable-level hook to DataProxy to provide the
  correlation matrix, i.e. dp.getCorrelationMatrix("path/to/MYOBS"), which
  should currently return an identity matrix of the correct size (i.e. #bins)
  for that observable. Maybe we could try updating the chi2 to use it. We should
  check whether this is going to slow down chi2 computation significantly: it's
  a borderline use-case, and maybe it's better to return None for the 99% of
  cases where there is no corr matrix and put a corresponding conditional in the
  chi2/DoF calcs. OR, also have a dp.hasCorrelationMatrix(obspath) function so
  that the chi2 calc can *choose* to be more efficient... I think I like that
  best.

  Update: I've been thinking about this a bit... the number of true DoF can be
  calculated iteratively from a bin-by-bin correlation matrix
  (db_i/dx)/(db_j/dx) * b_j/b_i for a "hidden parameter" x by starting from the
  assumption that all bins are independent and "worth" a whole unit of DoF, then
  stripping the dependency on each other parameter. Quite neat, not sure if it's
  standard or if there is a closed-form equivalent. Anyway, the effect would
  just be to reduce the DoF... whether the weighting should be changed across
  the histo I don't know... need to think about that.

* Try using GPU computation for parallelising the bin ipol building or
  multi-ipol minimising on a single machine. PyCUDA seems a very impressive
  package. This would be a nice MSc or summer student project.

* prof-plotpulls: have a Latex-string sanitizing method and/or write output suitable for
  make-plots, also, there seems to be a memory issue with this script
